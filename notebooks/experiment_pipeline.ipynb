{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mechanistic Interpretability of Jailbreak Prompts\n",
        "\n",
        "In this notebook, we'll explore how language models respond to various jailbreak prompts of different strengths compared to a \"clean\" dataset of prompts. The goal is to:\n",
        "- Load and store these prompts within Colab (mounted to Google Drive).\n",
        "- Potentially run inference on a model to analyze where \"jailbreak\" behavior arises.\n",
        "- Lay groundwork for mechanistic interpretability (e.g., analyzing specific model attention heads, hidden states, or other internal mechanisms).\n",
        "\n",
        "We will start small, with:\n",
        "1. Data loading (Google Drive).\n",
        "2. Basic data inspection and prompt categorization.\n",
        "3. Setting up inference code for a small language model (e.g., a Hugging Face model) as a test bed."
      ],
      "metadata": {
        "id": "P-4j4Pn6y8X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Python version (optional):\n",
        "import sys\n",
        "print(\"Python version:\", sys.version)\n",
        "\n",
        "# Get installations\n",
        "!pip install --quiet torch numpy matplotlib scikit-learn pandas\n",
        "!pip install --quiet huggingface_hub transformers\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# If you want to check GPU usage:\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzwGEhsNy7If",
        "outputId": "1be8b247-d2e1-402e-db47-12054bba5292"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RZL7yTqPuQdq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# After running this cell, follow the link to grant Colab access to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/IdaCy/jailbreak-alert-research.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf6GUKKxzL9j",
        "outputId": "95b69caf-ea69-4372-d15a-27cc2fe54231"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jailbreak-alert-research'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 184 (delta 68), reused 152 (delta 38), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (184/184), 190.68 KiB | 5.61 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1FOfovTzN8s",
        "outputId": "3c9ac498-fe7d-44be-9037-3d580b561275"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mjailbreak-alert-research\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd jailbreak-alert-research"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJMMD-7xzPUJ",
        "outputId": "dee5135f-70da-4937-8342-fadeb30577d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jailbreak-alert-research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ1asfUfzQmt",
        "outputId": "8fd6f552-ca4c-45a0-a582-5879466fa6e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub --quiet\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# This will prompt you in Colab to enter your HF token or log in directly\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "o1NKfbqazQ5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference on the **general jailbreak** prompts:"
      ],
      "metadata": {
        "id": "jfNMuioR0mGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_FILE = \"data/ReNeLLM/jailbreaks/jb400.csv\"\n",
        "OUTPUT_DIR = \"output/extractions/jailbreak\"\n",
        "\n",
        "MODEL_NAME = \"google/gemma-2-9b\"\n",
        "HF_TOKEN = None\n",
        "BATCH_SIZE = 2\n",
        "NUM_SAMPLES = 10\n",
        "\n",
        "%run -i scripts/gemma2b/inference.py"
      ],
      "metadata": {
        "id": "Xt0I9v68zScu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference on any **other** prompts:"
      ],
      "metadata": {
        "id": "11MC-nmk0rLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_FILE = \"data/ReNeLLM/neutral/neutr400.csv\"\n",
        "\n",
        "# ATTENTION: don't forget creating the folder first\n",
        "OUTPUT_DIR = \"output/extractions/neutral\"\n",
        "\n",
        "MODEL_NAME = \"google/gemma-2-9b\"\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "%run -i scripts/gemma2b/inference.py"
      ],
      "metadata": {
        "id": "b7vUUZSM0z1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference on any **other** prompts?"
      ],
      "metadata": {
        "id": "EZLZdMzY1CEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_FILE = \"data/ReNeLLM/jailbreak_stronger/jb400.csv\"\n",
        "\n",
        "# ATTENTION: don't forget creating the folder first\n",
        "OUTPUT_DIR = \"output/extractions/jailbreak_stronger\"\n",
        "\n",
        "MODEL_NAME = \"google/gemma-2-9b\"\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "%run -i scripts/gemma2b/inference.py"
      ],
      "metadata": {
        "id": "itv2Ic6s1EI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the differences & PCA of any two (or more!)"
      ],
      "metadata": {
        "id": "uoQW-O8H1Pm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "JB_DIR = \"output/extractions/jailbreak\"\n",
        "NEUTRAL_DIR = \"output/extractions/neutral\"\n",
        "DIFF_DIR = \"output/diff\"\n",
        "\n",
        "%run -i scripts/analyses/1_compute_diff.py"
      ],
      "metadata": {
        "id": "h0MgB5OMze87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIFF_DIR = \"output/differences\"\n",
        "OUTPUT_DIR = \"output/PCA\"\n",
        "\n",
        "%run -i scripts/analyses/2_run_pca.py"
      ],
      "metadata": {
        "id": "yaI8arFKzfXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_pt = \"output/PCA/layer_pca_results.pt\"\n",
        "PC1_FILE = \"output/PCA/layer_pc1_vectors.pt\"\n",
        "\n",
        "%run -i scripts/analyses/3_pca_check.py"
      ],
      "metadata": {
        "id": "zUzKB3Cqzgas"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}